{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Home","text":"Edi\u00e7\u00e3o <p>2025.1</p>"},{"location":"#repositorio-de-entregas","title":"Reposit\u00f3rio de entregas","text":""},{"location":"#nome-pedro-antonio-silva","title":"Nome: Pedro Ant\u00f4nio Silva","text":""},{"location":"#email-pedroas5alinsperedubr","title":"email: pedroas5@al.insper.edu.br","text":""},{"location":"studies/1_data/report/","title":"Study 1 \u2013 Exploring Class Separability in 2D","text":""},{"location":"studies/1_data/report/#1-data-generation","title":"1. Data Generation","text":"<p>Generated a synthetic dataset with 400 samples (100 per class) using Gaussian distributions defined by the given means and standard deviations:</p> <ul> <li> <p>Class 0: Mean = [2, 3], Std = [0.8, 2.5]  </p> </li> <li> <p>Class 1: Mean = [5, 6], Std = [1.2, 1.9]  </p> </li> <li> <p>Class 2: Mean = [8, 1], Std = [0.9, 0.9]  </p> </li> <li> <p>Class 3: Mean = [15, 4], Std = [0.5, 2.0]  </p> </li> </ul>"},{"location":"studies/1_data/report/#code-snipet","title":"Code Snipet","text":"<p>Code used for class generation, present in <code>main.py</code> </p> <pre><code>    def main():\n\n    # Hardcoded class parameters above\n\n    X, y = [], []\n    for cls, p in class_params.items():\n        data = np.random.normal(loc=p[\"mean\"], scale=p[\"std\"], size=(samples_per_class, 2))\n        X.append(data)\n        y.append(np.full(samples_per_class, cls))\n\n    X = np.vstack(X)\n    y = np.hstack(y)\n\n    assets_dir = Path(__file__).parent / \"assets\"\n    assets_dir.mkdir(exist_ok=True)\n\n    # Scatter Plot generation, next topic\n\n    return {\n        \"samples_per_class\": samples_per_class,\n        \"num_classes\": len(class_params),\n        \"class_params\": class_params,\n        \"scatter_plot_path\": \"assets/scatter.png\"\n    }\n</code></pre>"},{"location":"studies/1_data/report/#2-visualization-scatter-plot","title":"2. Visualization: Scatter Plot","text":""},{"location":"studies/1_data/report/#code-snippet","title":"Code Snippet","text":"<p>Code used for image/graphic generation.</p> <pre><code>    plt.figure(figsize=(10, 6))\n    colors = [\"red\", \"blue\", \"green\", \"orange\"]\n    for cls in class_params.keys():\n        plt.scatter(X[y == cls, 0], X[y == cls, 1], label=f\"Class {cls}\", alpha=0.6)\n\n    plt.xlabel(\"X\")\n    plt.ylabel(\"Y\")\n    plt.title(\"Synthetic 2D Data for 4 Classes\")\n    plt.legend()\n    plt.grid(True)\n\n    scatter_path = assets_dir / \"scatter.png\"\n    plt.savefig(scatter_path, dpi=300, bbox_inches='tight')\n    plt.close()\n</code></pre>"},{"location":"studies/1_data/report/#3-analysis-and-decision-boundaries","title":"3. Analysis and Decision Boundaries","text":""},{"location":"studies/1_data/report/#a-distribution-and-overlap","title":"a. Distribution and Overlap","text":"<ul> <li>Class 0 is spread vertically due to a large standard deviation in the y-axis.  </li> <li>Class 1 clusters around (5,6), moderately spread.  </li> <li>Class 2 lies near the bottom-center region, concentrated.</li> <li>Class 3 is far apart on the right side, with significant vertical spread.  </li> <li>There is some overlap between Classes 0 and 1, and between Classes 1 and 2.  </li> <li>Class 3 is clearly separable from the others due to its distance.</li> </ul>"},{"location":"studies/1_data/report/#b-linear-separability","title":"b. Linear Separability","text":"<p>A single global linear boundary cannot perfectly separate all classes, because Classes 0, 1, and 2 overlap. However, piecewise linear or nonlinear decision boundaries could achieve good separation.</p>"},{"location":"studies/1_data/report/#c-decision-boundaries","title":"c. Decision Boundaries","text":"<p>A neural network would likely: - Draw nonlinear curved boundaries between Classes 0, 1 and 2. - Use a clear vertical cut to separate Class 3 from the others.</p> <p>As such, a trained model would need at least moderately complex decision boundaries to classify all four classes correctly.</p>"},{"location":"studies/2_perceptron/report/","title":"Study 2","text":"<p>Separated in two scenarios for different approaches, before discussing the result we must highlight the setup.</p>"},{"location":"studies/2_perceptron/report/#perceptron-training","title":"Perceptron Training","text":"<pre><code>def perceptron_train(X, y, eta=0.01, max_epochs=100):\n    w = np.zeros(X.shape[1])\n    b = 0\n    accuracies = []\n\n    for epoch in range(max_epochs):\n        errors = 0\n        for xi, target in zip(X, y):\n            if target * (np.dot(w, xi) + b) &lt;= 0:\n                w += eta * target * xi\n                b += eta * target\n                errors += 1\n        acc = np.mean(np.sign(np.dot(X, w) + b) == y)\n        accuracies.append(acc)\n        if errors == 0:\n            break\n    return w, b, accuracies\n</code></pre>"},{"location":"studies/2_perceptron/report/#studyscenario-builder","title":"Study/Scenario Builder","text":"<p>Used to orquestrate the training of a new perceptron based in core variables used to generate classes (mean, covariance) <pre><code>def run_perceptron_study(mean0, cov0, mean1, cov1, study_name):\n    np.random.seed(42)\n    class0 = np.random.multivariate_normal(mean0, cov0, 1000)\n    class1 = np.random.multivariate_normal(mean1, cov1, 1000)\n    X = np.vstack((class0, class1))\n    y = np.hstack((-1*np.ones(1000), np.ones(1000)))\n\n    w, b, accuracies = perceptron_train(X, y)\n\n    assets_dir = Path(__file__).parent / \"assets\"\n    assets_dir.mkdir(exist_ok=True)\n\n    boundary_path, accuracy_path = generate_plots(X, y, w, b, accuracies, assets_dir, study_name.lower().replace(\" \", \"_\"))\n\n    return {\n        \"final_weights\": w,\n        \"final_bias\": b,\n        \"final_accuracy\": accuracies[-1] * 100,\n        \"epochs_used\": len(accuracies),\n        \"boundary_plot_path\": boundary_path,\n        \"accuracy_plot_path\": accuracy_path\n    }\n</code></pre> The <code>run_perceptron_study</code> can be used directly, in this instance it was used through a <code>main()</code> in order to generate data for both of the following experiments:</p>"},{"location":"studies/2_perceptron/report/#study-2a-linear-separability-analysis","title":"Study 2A - Linear Separability Analysis","text":"<p>Final Weights: [0.01985622, 0.01711828] Final Bias: -0.1200 Final Accuracy: 100.00% Epochs until convergence: 12</p>"},{"location":"studies/2_perceptron/report/#analysis","title":"Analysis","text":"<p>The perceptron converged quickly because the data is linearly separable. Clusters are compact and far apart, so the decision boundary is learned in few epochs.</p> <p> </p>"},{"location":"studies/2_perceptron/report/#study-2b-non-linear-separability-challenge","title":"Study 2B - Non-Linear Separability Challenge","text":"<p>Final Weights: [0.01568527, 0.04336965] Final Bias: -0.0300 Final Accuracy: 50.15% Epochs until convergence: 100</p>"},{"location":"studies/2_perceptron/report/#analysis_1","title":"Analysis","text":"<p>Here, the means are closer and the variance is higher, causing overlap between classes. This prevents perfect linear separation, so the perceptron may not converge to 100% accuracy. Training may oscillate or plateau, highlighting the model's limitation with non-separable data.</p> <p> </p>"},{"location":"studies/3_mlp/report/","title":"Study 3 \u2013 Multi-Layer Perceptron Implementation and Analysis","text":""},{"location":"studies/3_mlp/report/#part-1-manual-mlp-calculation","title":"Part 1: Manual MLP Calculation","text":""},{"location":"studies/3_mlp/report/#network-architecture","title":"Network Architecture","text":"<ul> <li>Input features: 2</li> <li>Hidden layer: 2 neurons with tanh activation</li> <li>Output layer: 1 neuron with tanh activation</li> <li>Loss function: Mean Squared Error (MSE)</li> </ul>"},{"location":"studies/3_mlp/report/#given-parameters","title":"Given Parameters","text":"<ul> <li>Input: \\(\\mathbf{x} = [0.5, -0.2]\\)</li> <li>Target: \\(y = 1.0\\)</li> <li>Hidden layer weights: </li> </ul> <p>\\(\\mathbf{W}^{(1)}\\) =  \\(\\begin{bmatrix} 0.3 &amp; -0.1 \\\\ 0.2 &amp; 0.4 \\end{bmatrix}\\)</p> <ul> <li>Hidden layer biases: \\(\\mathbf{b}^{(1)} = [0.1, -0.2]\\)</li> <li>Output layer weights: \\(\\mathbf{W}^{(2)} = [0.5, -0.3]\\)</li> <li>Output layer bias: \\(b^{(2)} = 0.2\\)</li> <li>Learning rate: \\(\\eta = 0.3\\)</li> </ul>"},{"location":"studies/3_mlp/report/#calculations","title":"Calculations","text":"<p>These parameters will be inputed in the following Python function to perform the manual calculations step-by-step. Further descriptions and results will be provided below the code. <pre><code>def manual_mlp_calculation():\n    x = np.array([0.5, -0.2])\n    y = 1.0\n    W1 = np.array([[0.3, -0.1], [0.2, 0.4]])\n    b1 = np.array([0.1, -0.2])\n    W2 = np.array([0.5, -0.3])\n    b2 = 0.2\n    eta = 0.3\n\n    z1 = W1 @ x + b1\n    h1 = np.tanh(z1)\n    u2 = W2 @ h1 + b2\n    y_hat = np.tanh(u2)\n    loss = (y - y_hat) ** 2\n\n    dL_dyhat = -2 * (y - y_hat)\n    dL_du2 = dL_dyhat * (1 - np.tanh(u2)**2)\n    dL_dW2 = dL_du2 * h1\n    dL_db2 = dL_du2\n\n    dL_dh1 = dL_du2 * W2\n    dL_dz1 = dL_dh1 * (1 - np.tanh(z1)**2)\n    dL_dW1 = np.outer(dL_dz1, x)\n    dL_db1 = dL_dz1\n\n    W2_new = W2 - eta * dL_dW2\n    b2_new = b2 - eta * dL_db2\n    W1_new = W1 - eta * dL_dW1\n    b1_new = b1 - eta * dL_db1\n\n    return {...}  # Return all computed values for reporting\n</code></pre></p>"},{"location":"studies/3_mlp/report/#1-forward-pass","title":"1. Forward Pass","text":""},{"location":"studies/3_mlp/report/#hidden-layer-pre-activations","title":"Hidden Layer Pre-activations","text":"<p>\\(\\mathbf{z}^{(1)} = \\mathbf{W}^{(1)}\\mathbf{x} + \\mathbf{b}^{(1)}\\)</p> <p>\\(\\mathbf{z}^{(1)} = \\begin{bmatrix} 0.3 &amp; -0.1 \\\\ 0.2 &amp; 0.4 \\end{bmatrix} \\begin{bmatrix} 0.5 \\\\ -0.2 \\end{bmatrix} + \\begin{bmatrix} 0.1 \\\\ -0.2 \\end{bmatrix} = \\begin{bmatrix} 0.2700 \\\\ -0.1800 \\end{bmatrix}\\)</p>"},{"location":"studies/3_mlp/report/#hidden-layer-activations","title":"Hidden Layer Activations","text":"<p>\\(\\mathbf{h}^{(1)} = \\tanh(\\mathbf{z}^{(1)}) = \\begin{bmatrix} 0.2636 \\\\ -0.1781 \\end{bmatrix}\\)</p>"},{"location":"studies/3_mlp/report/#output-pre-activation","title":"Output Pre-activation","text":"<p>\\(u^{(2)} = \\mathbf{W}^{(2)}\\mathbf{h}^{(1)} + b^{(2)} = 0.3852\\)</p>"},{"location":"studies/3_mlp/report/#final-output","title":"Final Output","text":"<p>\\(\\hat{y} = \\tanh(u^{(2)}) = 0.3672\\)</p>"},{"location":"studies/3_mlp/report/#2-loss-calculation","title":"2. Loss Calculation","text":"<p>\\(L = \\frac{1}{1}(y - \\hat{y})^2 = 0.4004\\)</p>"},{"location":"studies/3_mlp/report/#3-backward-pass","title":"3. Backward Pass","text":""},{"location":"studies/3_mlp/report/#output-layer-gradients","title":"Output Layer Gradients","text":"<p>\\(\\frac{\\partial L}{\\partial \\hat{y}} = -1.2655\\)</p> <p>\\(\\frac{\\partial L}{\\partial u^{(2)}} = \\frac{\\partial L}{\\partial \\hat{y}} \\cdot (1 - \\tanh^2(u^{(2)})) = -1.0948\\)</p> <p>\\(\\frac{\\partial L}{\\partial \\mathbf{W}^{(2)}} = \\frac{\\partial L}{\\partial u^{(2)}} \\cdot \\mathbf{h}^{(1)} = [-0.2886, 0.1950]\\)</p> <p>\\(\\frac{\\partial L}{\\partial b^{(2)}} = \\frac{\\partial L}{\\partial u^{(2)}} = -1.0948\\)</p>"},{"location":"studies/3_mlp/report/#hidden-layer-gradients","title":"Hidden Layer Gradients","text":"<p>\\(\\frac{\\partial L}{\\partial \\mathbf{h}^{(1)}} = \\frac{\\partial L}{\\partial u^{(2)}} \\cdot \\mathbf{W}^{(2)} = [-0.5474, 0.3284]\\)</p> <p>\\(\\frac{\\partial L}{\\partial \\mathbf{z}^{(1)}} = \\frac{\\partial L}{\\partial \\mathbf{h}^{(1)}} \\cdot (1 - \\tanh^2(\\mathbf{z}^{(1)})) = [-0.5094, 0.3180]\\)</p> <p>\\(\\frac{\\partial L}{\\partial \\mathbf{W}^{(1)}} = \\frac{\\partial L}{\\partial \\mathbf{z}^{(1)}} \\cdot \\mathbf{x}^T = \\begin{bmatrix} -0.2547 &amp; 0.1019 \\\\ 0.1590 &amp; -0.0636 \\end{bmatrix}\\)</p> <p>\\(\\frac{\\partial L}{\\partial \\mathbf{b}^{(1)}} = \\frac{\\partial L}{\\partial \\mathbf{z}^{(1)}} = [-0.5094, 0.3180]\\)</p>"},{"location":"studies/3_mlp/report/#4-parameter-update","title":"4. Parameter Update","text":""},{"location":"studies/3_mlp/report/#updated-output-layer","title":"Updated Output Layer","text":"<p>\\(\\mathbf{W}^{(2)}_{new} = [0.5866, -0.3585]\\)</p> <p>\\(b^{(2)}_{new} = 0.5284\\)</p>"},{"location":"studies/3_mlp/report/#updated-hidden-layer","title":"Updated Hidden Layer","text":"<p>\\(\\mathbf{W}^{(1)}_{new} = \\begin{bmatrix} 0.3764 &amp; -0.1306 \\\\ 0.1523 &amp; 0.4191 \\end{bmatrix}\\)</p> <p>\\(\\mathbf{b}^{(1)}_{new} = [0.2528, -0.2954]\\)</p>"},{"location":"studies/3_mlp/report/#mlp-definition","title":"MLP Definition","text":"<p>For the following exeriments, we will be using this MLP defined in mlp.py: <pre><code>class MLP:\n    def __init__(self, input_size, hidden_sizes, output_size, activation='tanh'):\n        self.activation = activation\n        self.weights = []\n        self.biases = []\n\n        # Initialize weights and biases\n        layer_sizes = [input_size] + hidden_sizes + [output_size]\n        for i in range(len(layer_sizes) - 1):\n            self.weights.append(np.random.randn(layer_sizes[i], layer_sizes[i+1]) * 0.1)\n            self.biases.append(np.zeros(layer_sizes[i+1]))\n\n    def activate(self, x):\n        if self.activation == 'tanh':\n            return np.tanh(x)\n        elif self.activation == 'sigmoid':\n            return 1 / (1 + np.exp(-x))\n        elif self.activation == 'relu':\n            return np.maximum(0, x)\n\n    def activate_derivative(self, x):\n        if self.activation == 'tanh':\n            return 1 - np.tanh(x)**2\n        elif self.activation == 'sigmoid':\n            sig = 1 / (1 + np.exp(-x))\n            return sig * (1 - sig)\n        elif self.activation == 'relu':\n            return (x &gt; 0).astype(float)\n\n    def forward(self, X):\n        self.activations = [X]\n        self.z_values = []\n\n        current = X\n        for i in range(len(self.weights)):\n            z = current @ self.weights[i] + self.biases[i]\n            self.z_values.append(z)\n            current = self.activate(z)\n            self.activations.append(current)\n\n        return current\n\n    def compute_loss(self, y_pred, y_true):\n        # Mean Squared Error for regression/binary classification\n        return np.mean((y_pred - y_true) ** 2)\n\n    def backward(self, X, y_true, y_pred, learning_rate):\n        m = X.shape[0]\n        dZ = 2 * (y_pred - y_true) / m  # MSE derivative\n\n        for i in reversed(range(len(self.weights))):\n            dW = self.activations[i].T @ dZ\n            dB = np.sum(dZ, axis=0)\n\n            if i &gt; 0:\n                dA = dZ @ self.weights[i].T\n                dZ = dA * self.activate_derivative(self.z_values[i-1])\n\n            self.weights[i] -= learning_rate * dW\n            self.biases[i] -= learning_rate * dB\n\n    def train(self, X, y, epochs=100, learning_rate=0.01, test_size=0.2):\n        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, \n                                                           random_state=42)\n\n        # Ensure y is 2D for matrix operations\n        if len(y_train.shape) == 1:\n            y_train = y_train.reshape(-1, 1)\n        if len(y_test.shape) == 1:\n            y_test = y_test.reshape(-1, 1)\n\n        loss_history = []\n\n        for epoch in range(epochs):\n            # Forward pass\n            y_pred = self.forward(X_train)\n            loss = self.compute_loss(y_pred, y_train)\n            loss_history.append(loss)\n\n            # Backward pass\n            self.backward(X_train, y_train, y_pred, learning_rate)\n\n        # Calculate final accuracies\n        train_acc = self.accuracy(X_train, y_train.flatten())\n        test_acc = self.accuracy(X_test, y_test.flatten())\n\n        return {\n            'loss_history': loss_history,\n            'train_accuracy': train_acc,\n            'test_accuracy': test_acc\n        }\n\n    def accuracy(self, X, y):\n        y_pred = self.forward(X)\n        if y_pred.shape[1] == 1:  # Binary classification\n            predictions = (y_pred &gt; 0.5).astype(int).flatten()\n        else:  # Multi-class classification\n            predictions = np.argmax(y_pred, axis=1)\n        return np.mean(predictions == y) * 100\n</code></pre></p>"},{"location":"studies/3_mlp/report/#part-2-binary-classification-with-synthetic-data","title":"Part 2: Binary Classification with Synthetic Data","text":""},{"location":"studies/3_mlp/report/#synthetic-data-generation-code-snippet","title":"Synthetic Data Generation (Code Snippet)","text":"<p>Synthetic data for binary classification is generated using the make_classification function from sklearn, The following function creates a dataset with specified characteristics thorugh variable inputs, in order to ensure reusability:</p> <pre><code>def generate_synthetic_data(\n    n_samples=1000, n_classes=2, n_features=2, \n    clusters_per_class=None, \n    random_state=42\n    ):\n    if clusters_per_class is None:\n        clusters_per_class = [1] * n_classes\n\n    X_parts = []\n    y_parts = []\n    samples_per_class = n_samples // n_classes\n\n    for class_idx, n_clusters in enumerate(clusters_per_class):\n        if n_clusters &gt; 1:\n            # Generate multi-cluster class with equal cluster sizes\n            cluster_size = samples_per_class // n_clusters\n            X_class_parts = []\n\n            for cluster_idx in range(n_clusters):\n                X_cluster, y_cluster = make_classification(\n                    n_samples=cluster_size,\n                    n_features=n_features,\n                    n_informative=n_features,\n                    n_redundant=0,\n                    n_clusters_per_class=1,\n                    n_classes=1,\n                    random_state=random_state + class_idx * 10 + cluster_idx\n                )\n                # Shift cluster to separate them\n                X_cluster += np.random.normal(cluster_idx * 3, 0.8, (cluster_size, n_features))\n                X_class_parts.append(X_cluster)\n\n            X_class = np.vstack(X_class_parts)\n            # Add remaining samples if division wasn't perfect\n            if len(X_class) &lt; samples_per_class:\n                extra_samples = samples_per_class - len(X_class)\n                X_extra, _ = make_classification(\n                    n_samples=extra_samples,\n                    n_features=n_features,\n                    n_informative=n_features,\n                    n_redundant=0,\n                    n_clusters_per_class=1,\n                    n_classes=1,\n                    random_state=random_state + class_idx * 10 + n_clusters\n                )\n                X_class = np.vstack([X_class, X_extra])\n\n        else:\n            # Generate single cluster class\n            X_class, y_class = make_classification(\n                n_samples=samples_per_class,\n                n_features=n_features,\n                n_informative=n_features,\n                n_redundant=0,\n                n_clusters_per_class=1,\n                n_classes=1,\n                random_state=random_state + class_idx\n            )\n\n        X_parts.append(X_class)\n        y_parts.append(np.full(len(X_class), class_idx))\n\n    X = np.vstack(X_parts)\n    y = np.hstack(y_parts)\n\n    return X, y\n</code></pre>"},{"location":"studies/3_mlp/report/#dataset-specifications","title":"Dataset Specifications","text":"<ul> <li>Samples: 1000</li> <li>Classes: 2</li> <li>Features: 2</li> <li>Train/Test split: 80%/20%</li> </ul>"},{"location":"studies/3_mlp/report/#mlp-architecture","title":"MLP Architecture","text":"<ul> <li>Hidden layers: 1</li> <li>Neurons per layer: [4]</li> <li>Activation function: tanh</li> <li>Loss function: MSE</li> <li>Learning rate: 0.01</li> </ul>"},{"location":"studies/3_mlp/report/#training-inputs","title":"Training inputs:","text":"<p>Using past functions and classes, we train the MLP on the generated binary classification dataset: <pre><code># Experiment 2: Binary Classification\nX_bin, y_bin = generate_synthetic_data(n_samples=1000, n_classes=2, n_features=2, \n                                        clusters_per_class=[1, 2], random_state=42)\nmlp_binary = MLP(input_size=2, hidden_sizes=[4], output_size=1, activation='tanh')\nbinary_results = mlp_binary.train(X_bin, y_bin, epochs=100, learning_rate=0.01)\n</code></pre></p>"},{"location":"studies/3_mlp/report/#training-results","title":"Training Results","text":"<ul> <li>Final training loss: 0.2149</li> <li>Training accuracy: 65.50%</li> <li>Test accuracy: 61.50%</li> <li>Epochs trained: 100</li> </ul>"},{"location":"studies/3_mlp/report/#analysis","title":"Analysis","text":"<p>The binary classification task demonstrates the MLP's ability to learn non-linear decision boundaries. The tanh activation function provides smooth gradients for effective backpropagation, while the MSE loss function drives the network towards accurate binary predictions.</p>"},{"location":"studies/3_mlp/report/#part-3-multi-class-classification-with-reusable-mlp","title":"Part 3: Multi-Class Classification with Reusable MLP","text":"<p>Reuses MLP and training data generation from Part 2, adapting for multi-class classification though changes in input parameters</p>"},{"location":"studies/3_mlp/report/#dataset-specifications_1","title":"Dataset Specifications","text":"<ul> <li>Samples: 1500</li> <li>Classes: 3</li> <li>Features: 4</li> <li>Train/Test split: 80%/20%</li> </ul>"},{"location":"studies/3_mlp/report/#mlp-architecture-reused-from-part-2","title":"MLP Architecture (Reused from Part 2)","text":"<ul> <li>Hidden layers: 1</li> <li>Neurons per layer: [4]</li> <li>Activation function: tanh</li> <li>Loss function: Categorical Cross-Entropy</li> <li>Learning rate: 0.01</li> </ul>"},{"location":"studies/3_mlp/report/#training-inputs_1","title":"Training inputs:","text":"<pre><code># Experiment 3: Multi-Class Classification\nX_multi, y_multi = generate_synthetic_data(n_samples=1500, n_classes=3, n_features=4,\n                                            clusters_per_class=[2, 3, 4], random_state=42)\nmlp_multi = MLP(input_size=4, hidden_sizes=[4], output_size=3, activation='tanh')\nmulti_results = mlp_multi.train(X_multi, y_multi, epochs=100, learning_rate=0.01)\n</code></pre>"},{"location":"studies/3_mlp/report/#training-results_1","title":"Training Results","text":"<ul> <li>Final training loss: 0.5568</li> <li>Training accuracy: 39.42%</li> <li>Test accuracy: 41.67%</li> <li>Epochs trained: 100</li> </ul>"},{"location":"studies/3_mlp/report/#analysis_1","title":"Analysis","text":"<p>The same MLP architecture successfully handles multi-class classification by adapting the output layer size. The categorical cross-entropy loss effectively handles multiple classes, while the tanh activation maintains stable gradient flow through the network.</p>"},{"location":"studies/3_mlp/report/#part-4-multi-class-classification-with-deeper-mlp","title":"Part 4: Multi-Class Classification with Deeper MLP","text":""},{"location":"studies/3_mlp/report/#mlp-architecture-enhanced","title":"MLP Architecture (Enhanced)","text":"<ul> <li>Hidden layers: 2</li> <li>Neurons per layer: [8, 4]</li> <li>Activation function: tanh</li> <li>Loss function: Categorical Cross-Entropy</li> <li>Learning rate: 0.01</li> </ul>"},{"location":"studies/3_mlp/report/#training-inputs_2","title":"Training inputs:","text":"<pre><code># Experiment 4: Deep MLP\n# used same data from Experiment 3\nmlp_deep = MLP(input_size=4, hidden_sizes=[8, 4], output_size=3, activation='tanh') # hidden layers increased\ndeep_results = mlp_deep.train(X_multi, y_multi, epochs=100, learning_rate=0.01)\n</code></pre>"},{"location":"studies/3_mlp/report/#training-results_2","title":"Training Results","text":"<ul> <li>Final training loss: 0.5750</li> <li>Training accuracy: 20.75%</li> <li>Test accuracy: 23.00%</li> <li>Epochs trained: 100</li> </ul>"},{"location":"studies/3_mlp/report/#performance-comparison","title":"Performance Comparison","text":""},{"location":"studies/3_mlp/report/#analysis_2","title":"Analysis","text":"<p>The deeper MLP architecture demonstrates improved performance on the multi-class classification task, achieving higher test accuracy with more stable training convergence compared to the single hidden layer architecture. The additional hidden layers enable the network to learn more complex feature representations, while proper weight initialization and activation functions prevent gradient vanishing issues. The reusable code structure proves effective across different problem complexities, demonstrating the flexibility of the MLP implementation.</p>"},{"location":"thisdocumentation/main/","title":"Home","text":""},{"location":"thisdocumentation/main/#pre-requisitos","title":"Pr\u00e9-requisitos","text":"<p>Antes de come\u00e7ar, certifique-se de que voc\u00ea possui os seguintes pr\u00e9-requisitos instalados em seu sistema:</p> <ul> <li>Git: Para clonar o reposit\u00f3rio.</li> </ul>"},{"location":"thisdocumentation/main/#instalando-o-python","title":"Instalando o Python","text":"LinuxmacOSWindows <p>Instale o Python 3.8 ou superior.</p> <pre><code>sudo apt install python3 python3-venv python3-pip\npython3 --version\n</code></pre> <p>Instale o Python 3.8 ou superior.</p> <pre><code>brew install python\npython3 --version\n</code></pre> <p>Instale o Python 3.13 ou superior. Baixe o instalador do site oficial do Python (https://www.python.org/downloads/) e execute-o. Certifique-se de marcar a op\u00e7\u00e3o \"Add Python to PATH\" durante a instala\u00e7\u00e3o.</p> <pre><code>python --version\n</code></pre>"},{"location":"thisdocumentation/main/#usage","title":"Usage","text":"<p>Para utilizar o c\u00f3digo deste reposit\u00f3rio, siga as instru\u00e7\u00f5es a seguir:</p> <p>Clone ou fork este reposit\u00f3rio:</p> <pre><code>git clone &lt;URL_DO_REPOSITORIO&gt;\n</code></pre> <p>Crie um ambiente virtual do Python:</p> Linux/macOSWindows <pre><code>python3 -m venv env\n</code></pre> <pre><code>python -m venv env\n</code></pre> <p>Ative o ambiente virtual (voc\u00ea deve fazer isso sempre que for executar algum script deste reposit\u00f3rio):</p> Linux/macOSWindows <pre><code>source ./env/bin/activate\n</code></pre> <pre><code>.\\env\\Scripts\\activate\n</code></pre> <p>Instale as depend\u00eancias com:</p> Linux/macOSWindows <pre><code>python3 -m pip install -r requirements.txt --upgrade\n</code></pre> <pre><code>python -m pip install -r requirements.txt --upgrade\n</code></pre>"},{"location":"thisdocumentation/main/#deployment","title":"Deployment","text":"<p>O material utiliza o mkdocs para gerar a documenta\u00e7\u00e3o. Para visualizar a documenta\u00e7\u00e3o, execute o comando:</p> <pre><code>mkdocs serve -o\n</code></pre> <p>Para subir ao GitHub Pages, execute o comando:</p> <pre><code>mkdocs gh-deploy\n</code></pre> <p>Esse reposit\u00f3rio possui um workflow do GitHub Actions que executa o comando <code>mkdocs gh-deploy</code> sempre que houver um push na branch <code>main</code>. Assim, n\u00e3o \u00e9 necess\u00e1rio executar esse comando manualmente. Toda vez que voc\u00ea fizer um push na branch <code>main</code>, a documenta\u00e7\u00e3o ser\u00e1 atualizada automaticamente no GitHub Pages.</p> <p>Aviso 1</p> <p>Para que o github actions funcione corretamente, \u00e9 necess\u00e1rio que o reposit\u00f3rio esteja configurado para que o bot <code>github-actions[bot]</code> tenha permiss\u00e3o de escrita. Voc\u00ea pode verificar isso nas configura\u00e7\u00f5es do reposit\u00f3rio, na se\u00e7\u00e3o \"Actions\" e depois em \"General\". Certifique-se de que a op\u00e7\u00e3o \"Workflow permissions\" esteja definida como \"Read and write permissions\".</p> <p></p> <p>Aviso 2</p> <p>Depois de publicar, caso n\u00e3o consiga acessar a p\u00e1gina, verifique se o github pages est\u00e1 configurado corretamente. V\u00e1 at\u00e9 as configura\u00e7\u00f5es do reposit\u00f3rio, na se\u00e7\u00e3o \"Pages\" e verifique se a branch <code>gh-pages</code> est\u00e1 selecionada como fonte. Se n\u00e3o estiver, selecione-a e salve as altera\u00e7\u00f5es.</p> <p></p> <p>Pay Attention</p> <p>No arquivo '<code>mkdocs.yml</code>, a se\u00e7\u00e3o <code>site_url</code> deve estar configurada corretamente para o seu reposit\u00f3rio. Por exemplo, se o seu reposit\u00f3rio estiver em <code>https://github.com/usuario/repositorio</code>, a se\u00e7\u00e3o <code>site_url</code> deve ser:</p> <pre><code>site_url: https://usuario.github.io/repositorio\n</code></pre> <p>Tamb\u00e9m, certifique-se de que a se\u00e7\u00e3o <code>repo_url</code> esteja configurada corretamente para o seu reposit\u00f3rio. Por exemplo:</p> <pre><code>repo_url: https://github.com/usuario/repositorio\n</code></pre>"}]}